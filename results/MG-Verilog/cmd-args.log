/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py trainer.log_every_n_steps=1 trainer.precision=bf16 trainer.devices=1 trainer.num_nodes=1 trainer.val_check_interval=20 trainer.max_steps=500 model.restore_from_path=./CodeLlama-7b.nemo model.peft.peft_scheme=lora model.micro_batch_size=1 model.global_batch_size=128 model.tensor_model_parallel_size=1 model.pipeline_model_parallel_size=1 model.megatron_amp_O2=True model.activations_checkpoint_granularity=selective model.activations_checkpoint_num_layers=null model.activations_checkpoint_method=uniform model.optim.name=fused_adam model.optim.lr=1e-4 model.answer_only_loss=True model.data.train_ds.file_names=[MG-Verilog/data_11k_train.jsonl] model.data.validation_ds.file_names=[MG-Verilog/data_11k_train.jsonl] model.data.test_ds.file_names=[MG-Verilog/data_11k_train.jsonl] model.data.train_ds.concat_sampling_probabilities=[1.0] model.data.train_ds.max_seq_length=2048 model.data.validation_ds.max_seq_length=2048 model.data.train_ds.micro_batch_size=1 model.data.train_ds.global_batch_size=128 model.data.validation_ds.micro_batch_size=1 model.data.validation_ds.global_batch_size=128 model.data.train_ds.num_workers=0 model.data.validation_ds.num_workers=0 model.data.test_ds.num_workers=0 model.data.validation_ds.metric.name=loss model.data.test_ds.metric.name=loss exp_manager.create_wandb_logger=False exp_manager.checkpoint_callback_params.mode=min exp_manager.explicit_log_dir=./results/MG-Verilog exp_manager.resume_if_exists=True exp_manager.resume_ignore_no_checkpoint=True exp_manager.create_checkpoint_callback=True exp_manager.checkpoint_callback_params.monitor=validation_loss ++exp_manager.checkpoint_callback_params.save_best_model=False exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True model.save_nemo_on_validation_end=False